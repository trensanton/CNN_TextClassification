{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_CNN_TextClassification_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f7729069d634476cbf8ab7e3b5ce7f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eab1d560d3794e31a256ccd619c1dc36",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b0e99232dbc84f45a561705c9bd364f9",
              "IPY_MODEL_cfe9d7e70cb7449ea9063efba82a8def"
            ]
          }
        },
        "eab1d560d3794e31a256ccd619c1dc36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0e99232dbc84f45a561705c9bd364f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8aa4e40c0b7346f09d615e389d92ebd4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a44320f421f345d6985603b3d0358e1b"
          }
        },
        "cfe9d7e70cb7449ea9063efba82a8def": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fcd61ea555654f8e9e7216e3015a178a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1999995/? [05:58&lt;00:00, 5581.59it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08bf663413464b9facf5845f047404e3"
          }
        },
        "8aa4e40c0b7346f09d615e389d92ebd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a44320f421f345d6985603b3d0358e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fcd61ea555654f8e9e7216e3015a178a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08bf663413464b9facf5845f047404e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC573xX1Ye1X"
      },
      "source": [
        "## Import libraries\n",
        "Here, we import the libraries required to develop our neural network model. Besides familiar libraries such as sklearn, nltk, we also import pytorch which is a deep learning library used for applications such as computer vision and natural language processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF0n-zJBAHSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb91adb2-2d4c-438d-9f2a-5f27d5e97ae9"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import f1_score, classification_report, roc_curve, auc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10FLLbd1ZOl5"
      },
      "source": [
        "## Upload data files and load data from csv to pandas dataframe\n",
        "Here, you upload the training and test files that we provide from your local machine to Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3UcSNMXEicE",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "ec22c31d-d4fb-4857-d4c3-25425e2a26c3"
      },
      "source": [
        "# Upload data files - note that it would take about 3 mins for the colab upload your files successfully\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a03cdde8-e606-4aa4-a9da-074f151cf094\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a03cdde8-e606-4aa4-a9da-074f151cf094\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving assignment5_processed_test.csv to assignment5_processed_test (1).csv\n",
            "Saving assignment5_processed_train.csv to assignment5_processed_train (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT2O2oGIHBmp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "d59aa781-3595-4f26-e0fc-30c624cd8325"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Import data into panda dataframes\n",
        "train_df = pd.read_csv(io.BytesIO(uploaded['assignment5_processed_train.csv']))\n",
        "test_df = pd.read_csv(io.BytesIO(uploaded['assignment5_processed_test.csv']))\n",
        "\n",
        "# Get only Text and Label columns for the task\n",
        "train_df = train_df[[\"ProcessedTweet\",\"Sentiment\"]]\n",
        "test_df = test_df[[\"ProcessedTweet\",\"Sentiment\"]]\n",
        "\n",
        "# Change name of the columns for convenience\n",
        "train_df.columns = [\"TEXT\",\"LABEL\"]\n",
        "test_df.columns = [\"TEXT\",\"LABEL\"]\n",
        "\n",
        "# convert labels to numeric values\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit([\"Positive\",\"Negative\",\"Neutral\"])\n",
        "print (\"List of labels: \", list(le.classes_))\n",
        "train_df.LABEL = le.transform(train_df.LABEL)\n",
        "test_df.LABEL = le.transform(test_df.LABEL)\n",
        "\n",
        "# Print the size of each set\n",
        "print (\"Training set: \", len(train_df))\n",
        "print (\"Test set: \", len(test_df))\n",
        "\n",
        "# Display the first 5 rows in each set for double-checking\n",
        "display(train_df.head(5))\n",
        "display(test_df.head(5))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of labels:  ['Negative', 'Neutral', 'Positive']\n",
            "Training set:  41142\n",
            "Test set:  3796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>LABEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http co ifz9fan2pa http co xx6ghgfzcc http co ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>advic talk neighbour famili exchang phone numb...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>coronaviru australia woolworth give elderli di...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>food stock one empti pleas panic enough food e...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>readi go supermarket outbreak paranoid food st...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                TEXT  LABEL\n",
              "0  http co ifz9fan2pa http co xx6ghgfzcc http co ...      1\n",
              "1  advic talk neighbour famili exchang phone numb...      2\n",
              "2  coronaviru australia woolworth give elderli di...      2\n",
              "3  food stock one empti pleas panic enough food e...      2\n",
              "4  readi go supermarket outbreak paranoid food st...      0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>LABEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>trend new yorker encount empti supermarket she...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>find hand sanit fred meyer turn 114 97 2 pack ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>find protect love one</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>buy hit citi anxiou shopper stock food amp med...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>australia updat gate one week everyon buy babi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                TEXT  LABEL\n",
              "0  trend new yorker encount empti supermarket she...      0\n",
              "1  find hand sanit fred meyer turn 114 97 2 pack ...      2\n",
              "2                              find protect love one      2\n",
              "3  buy hit citi anxiou shopper stock food amp med...      0\n",
              "4  australia updat gate one week everyon buy babi...      1"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLf6RJPHaEmw"
      },
      "source": [
        "## Check if GPU is available to run the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ9W7BI-6QT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556bee9f-f48e-4144-c2f3-71d7f94bd8e2"
      },
      "source": [
        "# Use cuda if present\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device available for running: \n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATt5AV0AYb5o"
      },
      "source": [
        "## Function 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW9U79aEJdAW"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "# define function\n",
        "def tokenize(texts):\n",
        "    max_len = 0\n",
        "    tokenized_texts = []\n",
        "    word2idx = {}\n",
        "\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "\n",
        "    idx = 2\n",
        "    for sent in texts:\n",
        "        tokenized_sent = word_tokenize(sent)\n",
        "\n",
        "        tokenized_texts.append(tokenized_sent)\n",
        "\n",
        "        for token in tokenized_sent:\n",
        "            if token not in word2idx:\n",
        "                word2idx[token] = idx\n",
        "                idx += 1\n",
        "\n",
        "        max_len = max(max_len, len(tokenized_sent))\n",
        "\n",
        "    return tokenized_texts, word2idx, max_len\n",
        "\n",
        "# Run the function\n",
        "all_text = train_df.TEXT.to_list() + test_df.TEXT.to_list()\n",
        "tokenized_texts, word2idx, max_len = tokenize(all_text)\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spnponq2MiOV"
      },
      "source": [
        "Input is TEXT in training set and test set.\n",
        "Output is tokenized texts in list, work frequencies, and maximum length of the longest text.\n",
        "This function is tokenizing the TEXT in both dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSqzrCiAct_0",
        "outputId": "e4338107-4ea4-4358-84ee-56de6f570337"
      },
      "source": [
        "print('Vocabulary Size: ', len(word2idx))\n",
        "print('bored:', word2idx['boredom'])\n",
        "print('panic:', word2idx['panic'])\n",
        "print('glad:', word2idx['glad'])\n",
        "print('safe:', word2idx['safe'])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size:  51361\n",
            "bored: 2466\n",
            "panic: 49\n",
            "glad: 2514\n",
            "safe: 56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-6Im_jSeCQi"
      },
      "source": [
        "## Function 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qUGT31lYadl"
      },
      "source": [
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "    input_ids = []\n",
        "    for tokenized_sent in tokenized_texts:\n",
        "\n",
        "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "\n",
        "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
        "        input_ids.append(input_id)\n",
        "    \n",
        "    return np.array(input_ids)\n",
        "\n",
        "# Run the function\n",
        "input_ids = encode(tokenized_texts, word2idx, max_len)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud3SIHYCoufk"
      },
      "source": [
        "Input is tokenized texts, dictionary which here is generated in function 1 - word2idx, and max_len in words - also generated in function 1.\n",
        "Output is an 1D array of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIeMnloTpzQe",
        "outputId": "d6bb7147-9eaf-4ba9-8aa5-244bf8da859d"
      },
      "source": [
        "len(tokenized_texts[0])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De66vHwyomob",
        "outputId": "36b81230-9d3a-4885-b2ec-e6645f7e0520"
      },
      "source": [
        "print(input_ids[0])\n",
        "print(len(input_ids[0]))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 3 4 2 3 5 2 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0]\n",
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg6vj2iXosX4"
      },
      "source": [
        "The shape of it is a 1D array, it is the index IDs in the word2idx dictionary, the rest of 0s are just filling ups because the length of this is shorter than the longest text of all tokenized texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A0y3SC0gOT-"
      },
      "source": [
        "## Download pre-trained word embeddings\n",
        "In this step, we are going to use fasttext pre-trained word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWmm5ec3M_pV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "377aea99-4f5e-4bcd-ab29-c467c87b041a"
      },
      "source": [
        "%%time\n",
        "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
        "FILE = \"fastText\"\n",
        "\n",
        "if os.path.isdir(FILE):\n",
        "    print(\"fastText exists.\")\n",
        "else:\n",
        "    !wget -P $FILE $URL\n",
        "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fastText exists.\n",
            "CPU times: user 917 µs, sys: 51 µs, total: 968 µs\n",
            "Wall time: 832 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o32QreZUg1rH"
      },
      "source": [
        "## Function 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0668goulLtPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "f7729069d634476cbf8ab7e3b5ce7f7d",
            "eab1d560d3794e31a256ccd619c1dc36",
            "b0e99232dbc84f45a561705c9bd364f9",
            "cfe9d7e70cb7449ea9063efba82a8def",
            "8aa4e40c0b7346f09d615e389d92ebd4",
            "a44320f421f345d6985603b3d0358e1b",
            "fcd61ea555654f8e9e7216e3015a178a",
            "08bf663413464b9facf5845f047404e3"
          ]
        },
        "outputId": "ab4d117d-0864-4652-8437-bf1d7c56583c"
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "\n",
        "def load_pretrained_vectors(word2idx, fname):\n",
        "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "\n",
        "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
        "\n",
        "    count = 0\n",
        "    for line in tqdm_notebook(fin):\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        word = tokens[0]\n",
        "        if word in word2idx:\n",
        "            count += 1\n",
        "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "    return embeddings\n",
        "  \n",
        "# Run the function\n",
        "embeddings = load_pretrained_vectors(word2idx, \"fastText/crawl-300d-2M.vec\")\n",
        "embeddings = torch.tensor(embeddings)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7729069d634476cbf8ab7e3b5ce7f7d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFKRL_CdyWB5"
      },
      "source": [
        "Inputs are dictionary and word to vector corpus filename - in here the imported fastText word to vector corpus, Output is embeddings of words. This function is using pretrained word vectors and embed them to the tokenized texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMefomE6MtOp",
        "outputId": "174ca4a9-8639-4ba0-e290-f933b408901c"
      },
      "source": [
        "embeddings[9]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.2667, -0.3146, -0.5417, -0.5040, -0.2145,  0.4093,  0.2662, -0.3960,\n",
              "        -0.0460, -0.3961,  0.0053,  0.4392,  0.3351,  0.0847, -0.6161, -0.3722,\n",
              "        -0.2911, -0.0162,  0.3547, -0.1321, -0.0685, -0.0682, -0.1115, -0.2473,\n",
              "         0.1107,  0.1364,  0.5421,  0.7690,  0.1951,  0.1173, -0.0965, -0.1923,\n",
              "        -0.0522, -0.2383, -0.4833, -0.5174, -0.1734,  0.3488, -0.1417, -0.0074,\n",
              "        -0.1720, -0.2842,  0.4342,  0.4815, -0.0183, -0.3316,  0.0451, -0.0840,\n",
              "        -0.0279, -0.2275,  0.2233, -0.1869,  0.2873,  0.0264,  0.0581,  0.0107,\n",
              "        -0.1087, -0.0400, -0.0673, -0.0021,  0.0355,  0.0558, -0.3831, -0.2310,\n",
              "        -0.2670, -0.3886, -0.0738,  0.2776,  0.0391, -0.3716, -0.0654, -0.1227,\n",
              "         0.0228,  0.1081, -0.3040, -0.0550, -0.2696, -0.1574, -0.0096,  0.2272,\n",
              "         0.0289,  0.1883,  0.2891,  0.4875,  0.1178,  0.2956,  0.3154,  0.0304,\n",
              "         0.0946, -0.5979,  0.4883,  0.2340, -0.3449, -0.1650,  0.0602, -0.3023,\n",
              "         0.2113,  0.3234,  0.1046, -0.0949, -0.0923,  0.1025, -0.0749,  0.0184,\n",
              "         0.1579, -0.1317,  0.2865, -0.1365,  0.4630, -0.0460,  0.0698, -0.2275,\n",
              "         0.1652,  0.1339, -0.2579, -0.0096,  0.1949,  0.2268, -0.1016, -0.2534,\n",
              "         0.3302, -0.4261,  0.3038,  0.0606,  0.0685,  0.5647,  0.2075,  0.1542,\n",
              "        -0.1301, -0.5023, -0.0156,  0.6032, -0.0316, -0.0226,  0.2614, -0.0506,\n",
              "         0.1789, -0.1054,  0.1979, -0.5495,  0.2092,  0.2560, -0.0896, -0.0576,\n",
              "        -0.0189,  0.0857,  0.1553, -0.1533,  0.3785, -0.1395,  0.3775,  0.1240,\n",
              "        -0.1162, -0.0228, -0.5460,  0.0484, -0.4205, -0.1213, -0.5067, -0.1014,\n",
              "         0.7802, -0.0105,  0.0049,  0.3803,  0.4184,  0.1003,  0.0516, -0.2863,\n",
              "         0.0615,  0.1859, -0.4523,  0.0040,  0.2583,  0.1383,  0.0128,  0.8895,\n",
              "        -0.1397,  0.1124, -0.0411,  0.0632, -0.0905, -0.1595,  0.1677, -0.4432,\n",
              "         0.1064,  0.0639, -0.0962,  0.2521, -0.2396, -0.1212, -0.0489,  0.2979,\n",
              "        -0.2551, -0.0403,  0.0083,  0.5047,  0.3059, -0.0779,  0.0166, -0.0971,\n",
              "        -0.1163,  0.0328, -0.2272,  0.1545,  0.0723,  0.3575, -0.1933, -0.0475,\n",
              "         0.0747, -0.3558,  0.0702,  0.6056, -0.0101, -0.2048, -0.0456, -0.0330,\n",
              "        -0.1796,  0.3180, -0.0476,  0.1350, -0.1015,  0.4290, -0.2540,  0.7816,\n",
              "         0.3415, -0.2052, -0.0257, -0.0379, -0.0931, -0.0897, -0.0483, -0.1890,\n",
              "        -0.1451,  0.0764, -0.1681, -0.2973,  0.1709, -0.4727,  0.1192, -0.0935,\n",
              "         0.1417,  0.2014,  0.2324, -0.3724, -0.5112, -0.2265,  0.0680, -0.0490,\n",
              "        -0.2225, -0.2647, -0.3880,  0.1655, -0.3490,  0.3137,  0.1352, -0.0086,\n",
              "         0.2941,  0.1910,  0.1951, -0.1463,  0.2105,  0.0672, -0.0238,  0.0903,\n",
              "        -0.2355, -0.1223, -0.1606,  0.2813,  0.3407, -0.3030, -0.1489,  0.0912,\n",
              "        -0.1207, -0.1482, -0.0391,  0.4992, -0.0301,  0.1454, -0.4092,  0.2770,\n",
              "        -0.1866,  0.0309,  0.2517, -0.1419, -0.0150, -0.1424, -0.0053,  0.4118,\n",
              "         0.3736,  0.4318,  0.2149,  0.0875, -0.0521,  0.1484, -0.0148,  0.0584,\n",
              "         0.3664,  0.3220,  0.3420, -0.3335], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmBJ516K7Q42"
      },
      "source": [
        "The shape is a 1D array of floats with length of 300. It indicates the vector of the 10th word in word2idx vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjYN9pZ7YSBY",
        "outputId": "4399f7d9-3aaf-487f-e3b4-7226811bf230"
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([51361, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3rByNm-nEcg"
      },
      "source": [
        "## Function 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2XdZKrUOkPU"
      },
      "source": [
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,SequentialSampler)\n",
        "\n",
        "def data_loader(train_inputs, test_inputs, train_labels, test_labels,\n",
        "                batch_size=50):\n",
        "\n",
        "    train_inputs, test_inputs, train_labels, test_labels = tuple(torch.tensor(data) for data in [train_inputs, test_inputs, train_labels, test_labels])\n",
        "\n",
        "    batch_size = 50\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    test_data = TensorDataset(test_inputs, test_labels)\n",
        "    test_sampler = SequentialSampler(test_data)\n",
        "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "# Run the function\n",
        "train_inputs = input_ids[:41142]\n",
        "test_inputs = input_ids[41142:]\n",
        "\n",
        "train_labels = train_df.LABEL.tolist()\n",
        "test_labels = test_df.LABEL.tolist()\n",
        "\n",
        "train_dataloader, test_dataloader = data_loader(train_inputs, test_inputs, train_labels, test_labels, batch_size=50)\n",
        "  "
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8qRdbUEYxhi"
      },
      "source": [
        "Inputs are training set, test set, and their labels.\n",
        "Outputs are prepared datasets for texts and corresponding labels.\n",
        "This function is loading data from training and testing sets and their labels and convert texts and labels into numbers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6V1Sz5knepG"
      },
      "source": [
        "## Function 5 - CNN Model\n",
        "In this section we are going to define a vanila CNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3BdcWuuq9Qy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_classifier(nn.Module):\n",
        "    def __init__(self,vocab_size=None,embed_dim=300,filter_sizes=2,num_filters=100,num_classes=3,dropout=0.5, learning_rate = 0.25):\n",
        "\n",
        "        super(CNN_classifier, self).__init__()\n",
        "\n",
        "        # Layer 1\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=self.embed_dim,padding_idx=0, max_norm=5.0)\n",
        "            \n",
        "        # Layer 2\n",
        "        self.conv1d_list = nn.ModuleList([nn.Conv1d(in_channels=self.embed_dim,out_channels=num_filters,kernel_size=filter_sizes)])\n",
        "        \n",
        "        # Layer 3\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "\n",
        "        # Layer 4\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in x_conv_list]\n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],dim=1)\n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWhHrMN6g80Z"
      },
      "source": [
        "Layer 1 is embedding layer which embed each word into a 1D array of 300 vectors. \n",
        "Layer 2 is convolutional layer that extract features from texts, then it slides the filter over texts to generate a 1D array.\n",
        "Layer 3 is fully converted layer, it classifies different classes by traininig.\n",
        "Layer 4 is dropout layer, to overcome the problem of overfitting, on passing a dropout of 0.5, 50% of the texts are dropped out randomly from the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afhDSJ3rqMTN"
      },
      "source": [
        "## Function 6:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWja5CovqXk6"
      },
      "source": [
        "import random\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, optimizer, train_dataloader, test_dataloader=None, epochs=10):\n",
        "    print(\"Start training...\\n\")\n",
        "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Test Loss':^10} | {'Test F1':^9}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    best_epoch = 0\n",
        "    best_f1 = 0\n",
        "    best_report = None\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "            model.zero_grad()\n",
        "            logits = model(b_input_ids)\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        if test_dataloader is not None:\n",
        "            test_loss, test_f1_score_mean = evaluate(model, test_dataloader)\n",
        "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {test_loss:^10.6f} | {test_f1_score_mean:^9.2f}\")\n",
        "            \n",
        "            if(test_f1_score_mean > best_f1):\n",
        "              best_f1 = test_f1_score_mean\n",
        "              best_epoch = epoch_i\n",
        "              best_report = classification_report(b_labels.tolist(), torch.argmax(logits, dim=1).flatten().tolist())\n",
        "    print('Best epoch the model performs: ', best_epoch)\n",
        "    print('Best performing model classification report: \\n', best_report)\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    val_f1_score = []\n",
        "    val_loss = []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids)\n",
        "\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        f1_score_item = f1_score(b_labels.cpu().numpy(),preds.cpu().numpy(), average=\"weighted\")\n",
        "        val_f1_score.append(f1_score_item)\n",
        "\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_f1_score_mean = np.mean(val_f1_score)\n",
        "\n",
        "    return val_loss, val_f1_score_mean"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suMkyqCaSy-V"
      },
      "source": [
        "For function train, the inputs are selected model, optimizer which is a hyperparameter in CNN model, training set (text + label), test set (text + label), and number of epochs which is the number of times the whole training set pass through the neural network. Output is a list of train loss, test loss, and test f1 score for each epoch. This function is training the input dataset using selected model.\n",
        "\n",
        "For function evaluate, the inputs are selected model, and value of dataset (text + label) so it can output the loss and f1 score. It is evaluating the performance of the training model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZftz2mLwnI2"
      },
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S8IgdvXzpqf"
      },
      "source": [
        "## Function 7 - Train and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBkj80NjuTX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad9ea7e-0389-486c-a749-ec93a955766a"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size=len(word2idx)\n",
        "embed_dim=300\n",
        "\n",
        "#filter_sizes=1\n",
        "filter_sizes=2\n",
        "#num_filters=100\n",
        "num_filters=200\n",
        "\n",
        "num_classes=3\n",
        "#dropout = 0.1\n",
        "dropout = 0.2\n",
        "\n",
        "#learning_rate = 0.01\n",
        "learning_rate = 0.0003\n",
        "\n",
        "cnn_model = CNN_classifier(vocab_size=vocab_size,\n",
        "                    embed_dim=embed_dim,\n",
        "                    num_classes= num_classes,\n",
        "                    filter_sizes = filter_sizes,\n",
        "                    num_filters = num_filters,\n",
        "                    dropout = dropout,\n",
        "                    learning_rate = learning_rate)\n",
        "\n",
        "optimizer = optim.Adam(cnn_model.parameters(),lr=learning_rate)\n",
        "    \n",
        "cnn_model.to(device)\n",
        "\n",
        "train(cnn_model, optimizer, train_dataloader, test_dataloader, epochs=20)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  | Test Loss  |  Test F1 \n",
            "--------------------------------------------------\n",
            "   1    |   0.829973   |  0.674513  |   0.72   \n",
            "   2    |   0.551050   |  0.546181  |   0.80   \n",
            "   3    |   0.417927   |  0.518702  |   0.81   \n",
            "   4    |   0.339732   |  0.517281  |   0.82   \n",
            "   5    |   0.271966   |  0.533123  |   0.81   \n",
            "   6    |   0.209138   |  0.563183  |   0.80   \n",
            "   7    |   0.160186   |  0.614263  |   0.79   \n",
            "   8    |   0.118180   |  0.663535  |   0.78   \n",
            "   9    |   0.087417   |  0.727516  |   0.77   \n",
            "  10    |   0.065455   |  0.787022  |   0.75   \n",
            "  11    |   0.050397   |  0.850490  |   0.74   \n",
            "  12    |   0.038644   |  0.912691  |   0.74   \n",
            "  13    |   0.031185   |  0.978975  |   0.73   \n",
            "  14    |   0.024999   |  1.029256  |   0.73   \n",
            "  15    |   0.021218   |  1.083438  |   0.73   \n",
            "  16    |   0.017074   |  1.131279  |   0.72   \n",
            "  17    |   0.013802   |  1.181611  |   0.72   \n",
            "  18    |   0.012680   |  1.246254  |   0.71   \n",
            "  19    |   0.010353   |  1.286180  |   0.72   \n",
            "  20    |   0.008841   |  1.325990  |   0.72   \n",
            "Best epoch the model performs:  3\n",
            "Best performing model classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89        19\n",
            "           1       1.00      1.00      1.00         5\n",
            "           2       0.89      0.89      0.89        18\n",
            "\n",
            "    accuracy                           0.90        42\n",
            "   macro avg       0.93      0.93      0.93        42\n",
            "weighted avg       0.90      0.90      0.90        42\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rZqKsoHeeY1"
      },
      "source": [
        "vocab_size is the length of dictionary (word2idx). embed_dim is the dimension of embedded vector space for all words in vocabulary. filter_size is the number of neighnor information you can see when processing the layer. num_filters is the number of neurons in the layer. dropout is what percentage is dropped out from the neural network. Learning rate helps to control how much to update the weight in the optimization algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV4r6Ozql_fd"
      },
      "source": [
        "Best f1 score is best for epoch 1. The larger the loss, the worse the F1 score. As Test loss reach maximum at around epoch 19, it performs the worest on F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqtN7TLO7uTQ"
      },
      "source": [
        "## Function 8:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RkBm7bW7xBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b15540d-d290-4017-892b-8c98192f92a3"
      },
      "source": [
        "def predict(text, model=cnn_model.to(\"cpu\"), max_len=62):\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n",
        "    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n",
        "\n",
        "    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n",
        "\n",
        "    logits = model.forward(input_id)\n",
        "\n",
        "    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n",
        "\n",
        "    print(f\"This review is {probs[0] * 100:.5f}% Negative;  {probs[1] * 100:.5f}% Neutral;  {probs[2] * 100:.5f}% Positive.\")\n",
        "\n",
        "predict(\"covid 19 is suck. I am fed up of staying at home.\")\n",
        "predict(\"I feel much better now since the vaccine has been produced.\")\n",
        "predict(\"Covid 19 is dangerous. I feel unsafe when going out these days.\")\n",
        "predict(\"It is good that the govenrment starts acting.\")"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This review is 100.00000% Negative;  0.00000% Neutral;  0.00000% Positive.\n",
            "This review is 0.00000% Negative;  0.00000% Neutral;  100.00000% Positive.\n",
            "This review is 25.00377% Negative;  24.76023% Neutral;  50.23600% Positive.\n",
            "This review is 0.00000% Negative;  0.00000% Neutral;  100.00000% Positive.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRKamAhwUfFE"
      },
      "source": [
        "Original:\n",
        "\n",
        "This review is 0.00126% Negative;  0.00000% Neutral;  99.99873% Positive.\n",
        "This review is 0.00000% Negative;  0.00000% Neutral;  100.00000% Positive.\n",
        "This review is 29.60551% Negative;  0.00074% Neutral;  70.39374% Positive.\n",
        "This review is 0.00000% Negative;  0.00000% Neutral;  100.00000% Positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puVXtIYKWTXh"
      },
      "source": [
        "After tuning:\n",
        "\n",
        "This review is 99.95264% Negative;  0.04655% Neutral;  0.00081% Positive.\n",
        "This review is 0.00000% Negative;  0.00001% Neutral;  99.99998% Positive.\n",
        "This review is 0.59901% Negative;  73.15240% Neutral;  26.24858% Positive.\n",
        "This review is 0.00001% Negative;  0.00000% Neutral;  99.99998% Positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDHaZ4jOWY4D"
      },
      "source": [
        "Yes, I did see improvements on the performance, I think the potential reason may be filter size, originally it was 1 so it cannot extract information from neighbors. As it can get information from neighbors after hyperparameter tuning, for the first prediction it turn out to be negative since \"fed up\" is very negative word combo. Originally when filter size is 1, it cannot detect it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUm_XgrCXmxW",
        "outputId": "da9daa0a-d24a-4d00-8fc2-c0e15dc8a46c"
      },
      "source": [
        "# My own tuning\n",
        "# Define hyperparameters\n",
        "vocab_size=len(word2idx)\n",
        "embed_dim=300\n",
        "\n",
        "#filter_sizes=1\n",
        "filter_sizes=2\n",
        "#num_filters=100\n",
        "num_filters=100\n",
        "\n",
        "num_classes=3\n",
        "#dropout = 0.1\n",
        "dropout = 0.3\n",
        "\n",
        "#learning_rate = 0.01\n",
        "learning_rate = 0.01\n",
        "\n",
        "cnn_model = CNN_classifier(vocab_size=vocab_size,\n",
        "                    embed_dim=embed_dim,\n",
        "                    num_classes= num_classes,\n",
        "                    filter_sizes = filter_sizes,\n",
        "                    num_filters = num_filters,\n",
        "                    dropout = dropout,\n",
        "                    learning_rate = learning_rate)\n",
        "\n",
        "optimizer = optim.Adam(cnn_model.parameters(),lr=learning_rate)\n",
        "    \n",
        "cnn_model.to(device)\n",
        "\n",
        "train(cnn_model, optimizer, train_dataloader, test_dataloader, epochs=20)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  | Test Loss  |  Test F1 \n",
            "--------------------------------------------------\n",
            "   1    |   0.711043   |  0.593878  |   0.79   \n",
            "   2    |   0.558129   |  0.726199  |   0.74   \n",
            "   3    |   0.368194   |  0.915208  |   0.69   \n",
            "   4    |   0.300668   |  1.150014  |   0.63   \n",
            "   5    |   0.287210   |  1.196869  |   0.64   \n",
            "   6    |   0.273318   |  1.205637  |   0.63   \n",
            "   7    |   0.280296   |  1.363516  |   0.63   \n",
            "   8    |   0.283924   |  1.831694  |   0.58   \n",
            "   9    |   0.267789   |  1.529301  |   0.63   \n",
            "  10    |   0.268250   |  1.671096  |   0.57   \n",
            "  11    |   0.258975   |  1.690187  |   0.61   \n",
            "  12    |   0.249657   |  1.860120  |   0.60   \n",
            "  13    |   0.248953   |  2.275559  |   0.58   \n",
            "  14    |   0.260522   |  2.056852  |   0.64   \n",
            "  15    |   0.250883   |  1.519942  |   0.63   \n",
            "  16    |   0.234276   |  1.914752  |   0.60   \n",
            "  17    |   0.231214   |  1.638235  |   0.65   \n",
            "  18    |   0.234306   |  2.241933  |   0.59   \n",
            "  19    |   0.236147   |  2.004184  |   0.65   \n",
            "  20    |   0.231837   |  4.259876  |   0.54   \n",
            "Best epoch the model performs:  0\n",
            "Best performing model classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.59      0.69        17\n",
            "           1       0.71      0.71      0.71         7\n",
            "           2       0.70      0.89      0.78        18\n",
            "\n",
            "    accuracy                           0.74        42\n",
            "   macro avg       0.75      0.73      0.73        42\n",
            "weighted avg       0.75      0.74      0.73        42\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-EYxe3kXfEp"
      },
      "source": [
        "My own tuning:\n",
        "\n",
        "This review is 100.00000% Negative;  0.00000% Neutral;  0.00000% Positive.\n",
        "This review is 0.00000% Negative;  0.00000% Neutral;  100.00000% Positive.\n",
        "This review is 25.00377% Negative;  24.76023% Neutral;  50.23600% Positive.\n",
        "This review is 0.00000% Negative;  0.00000% Neutral;  100.00000% Positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ-YVgwRXdVP"
      },
      "source": [
        "I chose filter_size = 2 because 1 performs bad in the first case, and I chose num_filters = 100 because the maximum text length is 62 which is less than 100. dropout = 0.3 so it can drop out some useless information. learning rate = 0.1 so it can learn some problems gradually at a not very fast speed to avoid quick converge caused by large number of learning rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv7yzwBBgnU1"
      },
      "source": [
        "Task 9:\n",
        "\n",
        "Inputs are text, and model used to predict. Output is prediction among positive, neutral, and negative. This function shows the probability for each sentiment of a text. These results are as expected even there are some predictions are not very accurate. But under most cases, it has a pretty good precision and recall rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsbS1hhv5MS6"
      },
      "source": [
        "## Exporting your results to PDF\n",
        "1. Download your notebook with _File -> Download .ipynb_\n",
        "1. Rename with your name like in other assignments, for example lastname_firstname_assignment5.ipynb\n",
        "1. Submit the notebook file on Moodle"
      ]
    }
  ]
}