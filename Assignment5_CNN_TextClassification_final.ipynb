{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_CNN_TextClassification_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC573xX1Ye1X"
      },
      "source": [
        "## Import libraries\n",
        "Here, we import the libraries required to develop our neural network model. Besides familiar libraries such as sklearn, nltk, we also import pytorch which is a deep learning library used for applications such as computer vision and natural language processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF0n-zJBAHSY"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import f1_score, classification_report, roc_curve, auc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10FLLbd1ZOl5"
      },
      "source": [
        "## Upload data files and load data from csv to pandas dataframe\n",
        "Here, you upload the training and test files that we provide from your local machine to Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3UcSNMXEicE"
      },
      "source": [
        "# Upload data files - note that it would take about 3 mins for the colab upload your files successfully\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT2O2oGIHBmp"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Import data into panda dataframes\n",
        "train_df = pd.read_csv(io.BytesIO(uploaded['assignment5_processed_train.csv']))\n",
        "test_df = pd.read_csv(io.BytesIO(uploaded['assignment5_processed_test.csv']))\n",
        "\n",
        "# Get only Text and Label columns for the task\n",
        "train_df = train_df[[\"ProcessedTweet\",\"Sentiment\"]]\n",
        "test_df = test_df[[\"ProcessedTweet\",\"Sentiment\"]]\n",
        "\n",
        "# Change name of the columns for convenience\n",
        "train_df.columns = [\"TEXT\",\"LABEL\"]\n",
        "test_df.columns = [\"TEXT\",\"LABEL\"]\n",
        "\n",
        "# convert labels to numeric values\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit([\"Positive\",\"Negative\",\"Neutral\"])\n",
        "print (\"List of labels: \", list(le.classes_))\n",
        "train_df.LABEL = le.transform(train_df.LABEL)\n",
        "test_df.LABEL = le.transform(test_df.LABEL)\n",
        "\n",
        "# Print the size of each set\n",
        "print (\"Training set: \", len(train_df))\n",
        "print (\"Test set: \", len(test_df))\n",
        "\n",
        "# Display the first 5 rows in each set for double-checking\n",
        "display(train_df.head(5))\n",
        "display(test_df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLf6RJPHaEmw"
      },
      "source": [
        "## Check if GPU is available to run the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ9W7BI-6QT9"
      },
      "source": [
        "# Use cuda if present\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATt5AV0AYb5o"
      },
      "source": [
        "## Function 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW9U79aEJdAW"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "# define function\n",
        "def tokenize(texts):\n",
        "    max_len = 0\n",
        "    tokenized_texts = []\n",
        "    word2idx = {}\n",
        "\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "\n",
        "    idx = 2\n",
        "    for sent in texts:\n",
        "        tokenized_sent = word_tokenize(sent)\n",
        "\n",
        "        tokenized_texts.append(tokenized_sent)\n",
        "\n",
        "        for token in tokenized_sent:\n",
        "            if token not in word2idx:\n",
        "                word2idx[token] = idx\n",
        "                idx += 1\n",
        "\n",
        "        max_len = max(max_len, len(tokenized_sent))\n",
        "\n",
        "    return tokenized_texts, word2idx, max_len\n",
        "\n",
        "# Run the function\n",
        "all_text = train_df.TEXT.to_list() + test_df.TEXT.to_list()\n",
        "tokenized_texts, word2idx, max_len = tokenize(all_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-6Im_jSeCQi"
      },
      "source": [
        "## Function 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qUGT31lYadl"
      },
      "source": [
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "    input_ids = []\n",
        "    for tokenized_sent in tokenized_texts:\n",
        "\n",
        "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "\n",
        "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
        "        input_ids.append(input_id)\n",
        "    \n",
        "    return np.array(input_ids)\n",
        "\n",
        "# Run the function\n",
        "input_ids = encode(tokenized_texts, word2idx, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A0y3SC0gOT-"
      },
      "source": [
        "## Download pre-trained word embeddings\n",
        "In this step, we are going to use fasttext pre-trained word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWmm5ec3M_pV"
      },
      "source": [
        "%%time\n",
        "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
        "FILE = \"fastText\"\n",
        "\n",
        "if os.path.isdir(FILE):\n",
        "    print(\"fastText exists.\")\n",
        "else:\n",
        "    !wget -P $FILE $URL\n",
        "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o32QreZUg1rH"
      },
      "source": [
        "## Function 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0668goulLtPp"
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "\n",
        "def load_pretrained_vectors(word2idx, fname):\n",
        "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "\n",
        "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
        "\n",
        "    count = 0\n",
        "    for line in tqdm_notebook(fin):\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        word = tokens[0]\n",
        "        if word in word2idx:\n",
        "            count += 1\n",
        "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "    return embeddings\n",
        "  \n",
        "# Run the function\n",
        "embeddings = load_pretrained_vectors(word2idx, \"fastText/crawl-300d-2M.vec\")\n",
        "embeddings = torch.tensor(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3rByNm-nEcg"
      },
      "source": [
        "## Function 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2XdZKrUOkPU"
      },
      "source": [
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,SequentialSampler)\n",
        "\n",
        "def data_loader(train_inputs, test_inputs, train_labels, test_labels,\n",
        "                batch_size=50):\n",
        "\n",
        "    train_inputs, test_inputs, train_labels, test_labels = tuple(torch.tensor(data) for data in [train_inputs, test_inputs, train_labels, test_labels])\n",
        "\n",
        "    batch_size = 50\n",
        "\n",
        "    train_data = TensorDataset(train_inputs, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    test_data = TensorDataset(test_inputs, test_labels)\n",
        "    test_sampler = SequentialSampler(test_data)\n",
        "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "# Run the function\n",
        "train_inputs = input_ids[:41142]\n",
        "test_inputs = input_ids[41142:]\n",
        "\n",
        "train_labels = train_df.LABEL.tolist()\n",
        "test_labels = test_df.LABEL.tolist()\n",
        "\n",
        "train_dataloader, test_dataloader = data_loader(train_inputs, test_inputs, train_labels, test_labels, batch_size=50)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6V1Sz5knepG"
      },
      "source": [
        "## Function 5 - CNN Model\n",
        "In this section we are going to define a vanila CNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3BdcWuuq9Qy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_classifier(nn.Module):\n",
        "    def __init__(self,vocab_size=None,embed_dim=300,filter_sizes=2,num_filters=100,num_classes=3,dropout=0.5, learning_rate = 0.25):\n",
        "\n",
        "        super(CNN_classifier, self).__init__()\n",
        "\n",
        "        # Layer 1\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=self.embed_dim,padding_idx=0, max_norm=5.0)\n",
        "            \n",
        "        # Layer 2\n",
        "        self.conv1d_list = nn.ModuleList([nn.Conv1d(in_channels=self.embed_dim,out_channels=num_filters,kernel_size=filter_sizes)])\n",
        "        \n",
        "        # Layer 3\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "\n",
        "        # Layer 4\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in x_conv_list]\n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],dim=1)\n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afhDSJ3rqMTN"
      },
      "source": [
        "## Function 6:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWja5CovqXk6"
      },
      "source": [
        "import random\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, optimizer, train_dataloader, test_dataloader=None, epochs=10):\n",
        "    print(\"Start training...\\n\")\n",
        "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Test Loss':^10} | {'Test F1':^9}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "            model.zero_grad()\n",
        "            logits = model(b_input_ids)\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        if test_dataloader is not None:\n",
        "            test_loss, test_f1_score_mean = evaluate(model, test_dataloader)\n",
        "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {test_loss:^10.6f} | {test_f1_score_mean:^9.2f}\")\n",
        "            \n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    val_f1_score = []\n",
        "    val_loss = []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids)\n",
        "\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        f1_score_item = f1_score(b_labels.cpu().numpy(),preds.cpu().numpy(), average=\"weighted\")\n",
        "        val_f1_score.append(f1_score_item)\n",
        "\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_f1_score_mean = np.mean(val_f1_score)\n",
        "\n",
        "    return val_loss, val_f1_score_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S8IgdvXzpqf"
      },
      "source": [
        "## Function 7 - Train and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBkj80NjuTX5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size=len(word2idx)\n",
        "embed_dim=300\n",
        "filter_sizes=1\n",
        "num_filters=100\n",
        "num_classes=3\n",
        "dropout = 0.1\n",
        "learning_rate = 0.01\n",
        "\n",
        "cnn_model = CNN_classifier(vocab_size=vocab_size,\n",
        "                    embed_dim=embed_dim,\n",
        "                    num_classes= num_classes,\n",
        "                    filter_sizes = filter_sizes,\n",
        "                    num_filters = num_filters,\n",
        "                    dropout = dropout,\n",
        "                    learning_rate = learning_rate)\n",
        "\n",
        "optimizer = optim.Adam(cnn_model.parameters(),lr=learning_rate)\n",
        "    \n",
        "cnn_model.to(device)\n",
        "\n",
        "train(cnn_model, optimizer, train_dataloader, test_dataloader, epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqtN7TLO7uTQ"
      },
      "source": [
        "## Function 8:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RkBm7bW7xBS"
      },
      "source": [
        "def predict(text, model=cnn_model.to(\"cpu\"), max_len=62):\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n",
        "    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n",
        "\n",
        "    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n",
        "\n",
        "    logits = model.forward(input_id)\n",
        "\n",
        "    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n",
        "\n",
        "    print(f\"This review is {probs[0] * 100:.5f}% Negative;  {probs[1] * 100:.5f}% Neutral;  {probs[2] * 100:.5f}% Positive.\")\n",
        "\n",
        "predict(\"covid 19 is suck. I am fed up of staying at home.\")\n",
        "predict(\"I feel much better now since the vaccine has been produced.\")\n",
        "predict(\"Covid 19 is dangerous. I feel unsafe when going out these days.\")\n",
        "predict(\"It is good that the govenrment starts acting.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsbS1hhv5MS6"
      },
      "source": [
        "## Exporting your results to PDF\n",
        "1. Download your notebook with _File -> Download .ipynb_\n",
        "1. Rename with your name like in other assignments, for example lastname_firstname_assignment5.ipynb\n",
        "1. Submit the notebook file on Moodle"
      ]
    }
  ]
}